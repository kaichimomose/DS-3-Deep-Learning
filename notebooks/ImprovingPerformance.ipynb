{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/ms_logo.jpeg' height=60% width=60%></center>\n",
    "\n",
    "\n",
    "<center><h1>Techniques For Improving Neural Network Performance</h1></center>\n",
    "\n",
    "In the first three notebooks, we learned about the basics of building a deep neural network, and a litte bit about the basic architecture.  We learned about the history of Single-Layer Perceptrons, what each neuron actually calculates, the different activation functions we can use to capture non-linearity in our model, and finally about the almight backprop algorithm that allowed our Multilayer Perceptron to \"learn\" the proper values for every parameter in the model, even though all those weights and biases started out as random numbers.\n",
    "\n",
    "In this notebook, we'll learn about a few different small-but-handy tricks that can make our models more more accurate, faster to train, and more resilient to overfitting.  Let's briefly examine the topics we'll cover in this notebook:\n",
    "\n",
    "<center><h3>1. Dropout Layers</h3></center>\n",
    "\n",
    "**_Dropout_** is a **_regularization technique_** designed to help prevent the model from **_overfitting_**.  It works by giving every neuron in a given layer a dropout percentage, which is the percentage chance that the neuron will be turned off rather than passing along it's input.  In this lesson, we'll learn about how this technique forces our neural networks to learn to be more resilient by probabilistically \"killing off\" neurons during training.\n",
    "\n",
    "<center><h3>2. Hyperparameter Tuning</h3></center>\n",
    "\n",
    "**_Hyperparameter Tuning_** is the hardest of the three topics we'll cover in this notebook.  Just as we used GridSearchCV to find the best combination of parameters for our models in DS2, we'll explore strategies for Hyperparameter Tuning to find the best combination for our Neural Networks in this notebook.  You'll likely find hyperparameter tuning for DS3 much harder and more time consuming than it was in DS2--neural networks have many more hyperparameters, and take longer to train, which means that combinatoric approaches like GridSearch are usually untenable. In this notebook, we'll explore the hyperparameters we should tune, and strategies for how to do so. \n",
    "\n",
    "\n",
    "<center><h2>Getting Started: Underfitting and Overfitting</h2></center>\n",
    "\n",
    "In DS2, we learned about how to evaluate the **_fit_** of our models.  Recall that in any supervised learning scenario, we split our data into training and testing sets.  Our training set is used to help our model learn insights from the data that it will use to make inferences to predict a class (classification tasks) or continue value (regression tasks).  Ideally, the model has learned just enough information from the training data to be generalizable to the real world.  If the model did learn enough about the training data to accurately make predictions, we say that the model has **_underfit_** the data.  If the model has mistaken variance in the training data for information it can use to make predictions, it will not generalize well to the real world.  Underfitting as if your model is blind to the answers in front it--overfitting is as if it is hallucinating.  \n",
    "<br>\n",
    "<center>Let's examine an analogy to help us better understand overfitting and underfitting: studying for exams with a practice test book!</center>\n",
    "<br>\n",
    "<center><img src='img/study_guide.jpg' height=75% width=75%></center>\n",
    "\n",
    "<h3>Study Guides Analogy</h3> \n",
    "\n",
    "**_Scenario_**\n",
    "You're a star student in high school, and you have a big test coming up in an AP (or IB, whichever you prefer) test coming up.  In order to study for the test, you buy two study books, like the one seen in the picture above.  These study books contain test questions that are a **_representative sample_** of the questions you'll see on the actual test.  They cover the same material, have the same format, and were probably written by the same people.  More importantly, all of the questions in these study books also come with with labeled answers for each question in the back of the book, so that you can check your work.  In order to mix it up, you pool all the questions from both books into one giant test (our **_training set_**), and then randomly set aside 25% of the practice questions (our **_test set_**).  \n",
    "\n",
    "Your study method is as follows:\n",
    "\n",
    "<center>**_Training_**</center> \n",
    "<ol>\n",
    "    <li>  Grab a question from your training set and try to answer it as if you were taking the real test. (**_forward propagation_**)</li>\n",
    "    <li> Check the answer in the back of the book to see if you got the question right or wrong.</li>\n",
    "<ul><li style='margin-top: -15px'>If you got it right, do nothing. You clearly already know this part of the material. No adjustment needed.</li>\n",
    "<li>If you got it wrong, study that question and try to learn from your mistake. (**_back propagation_**)  </li>\n",
    "</ul>\n",
    "<li>During this process, keep a running total of your grade on the training set.</li>\n",
    "</ol>\n",
    "When you've worked through all the questions in the training set, it's time to do a sanity check and make sure you actually know the material as well as you think you do.\n",
    "\n",
    "<center>**_Testing_**</center>\n",
    "\n",
    "Next, you sit down and answer all the questions in your testing set start to finish.  When you've finished, you flip to the back of the book to the answers to grade yourself.  You now have two grades to compare--your **_training score_**, and your **_testing score_**.    \n",
    "<br>\n",
    "<center>Let's examine three different possible scenarios that could arise:</center>\n",
    "<br>\n",
    "<center><img src='img/scenario_table.jpg' height=35% width=35%></center>\n",
    "\n",
    "**_Scenario 1: Underfitting:_**  If our score on both the training and testing sets are low, we can probably assume that our understanding of the material (our _model_) is lacking.  We don't yet know the material as well as we should, or we are biased towards what we think is correct, rather than what the data shows us is correct.  We can remedy this in a couple ways: \n",
    "\n",
    "#### Remedies:\n",
    "1. Spending more time training by working through our training data--maybe you didn't practice enough!\n",
    "1. Increase the complexity of your model--maybe you have an oversimplified view of the material.\n",
    "\n",
    "**_Scenario 2: Good Fit:_** If we score pretty much the same on our training and testing sets, and the scores are good scores, we can be confident that we actually understand the material that will be on the real AP Test.  We're not _guaranteed_ that we will do well on the test, but based on the information that we had to work with (the data contained within our practice test books), we can assume that we're as prepared as we possibly can be.  \n",
    "\n",
    "#### Remedies:\n",
    "None.  \"It ain't broke, so don't fix it!\" --Galileo (probably)\n",
    "\n",
    "**_Scenario 3: Overfitting:_** In this scenario, we can see that our training score is significantly higher than our testing score.  This is probably because we have accidentally \"discovered\" something in our training data that only applies to our training data through coincidence, or we've been through the training questions so many times that we have inadverently memorized the answers to each question without actually understanding _why_ that is the correct answer.  There are several ways we can remedy this as well:\n",
    "\n",
    "#### Remedies:\n",
    "\n",
    "1. Get more training data.  The more practice questions we have to go through, the less likely we are to mistake coincidence for something that will help us on the test.  For example, if we only had 10 questions, it's completely possible that we could mistakenly think we've discovered some random \"trick\" that helps us score well on the practice questions, but doesn't actually work on test questions in the real world (e.g. \"In the training data, I noticed that the answer to the even-numbered questions is 'B'!\").  The more data you have, the more counterexamples you'll have that will make it harder to accidentally \"learn\" incorrect patterns in the data.  \n",
    "1. Use regularization to simplify your model.  Simpler is better.  We've all gotten questions wrong before because we overthought.  Simpler models are always preferable to more complex models (as long as the models aren't so simple that they _underfit_ the data).  Regularization adds a penalty for complexity, reducing the overall complexity and making things simpler.  In this analogy, maybe you're considering 10 different things when you answer a question. If this is too complex, it might not generalize well to the real world test.  In that case, try only considering 5 or 6 things on each question, and see if it improves your score.  \n",
    "\n",
    "<center><h3>Evaluating Model Fitness in Keras</h3></center>\n",
    "\n",
    "Enough analogies--let's build a small neural network and try to evaluate model fitness using Tensorboard!\n",
    "\n",
    "**_Run the cell below to set up import everything that's needed to build a small neural network and preprocess the MNIST data set._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(60000, 784).astype('float32')\n",
    "X_test = X_test.reshape(10000, 784).astype('float32')\n",
    "X_train /= 255.\n",
    "X_test /= 255.\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a [Tensorboard callback]() by following the Keras documentation steps.  \n",
    "\n",
    "<center>Visualizing Results With Tensorboard</center>\n",
    "\n",
    "You've probably noticed by now that Keras is built on top of TensorFlow.  One of the most useful tools Google included in TensorFlow is Tensorboard.  Tensorboard is a way to keep track of things as your model trains, and visualize them in a handy dashboard. Since Keras is built on top of TensorFlow, its a cinch to use Tensorboard with our models! \n",
    "\n",
    "Next, we'll create examples of models that are overfit, underfit, and well fit.  During training, we'll keep track of metrics such as loss and accuracy, and visualize them in Tensorboard.  We'll then examine these visualizations to see how we can easily evaluate the fitness of our model.  \n",
    "\n",
    "#### Using Tensorboard\n",
    "\n",
    "To use Tensorboard, we'll follow these few easy steps:\n",
    "\n",
    "1. Create a Tensorboard Callback.  During this step, we can also tell the callback exactly what we do and don't want to track.  \n",
    "1. Pass the Tensorboard Callback object to the `callback=` parameter when we call `model.fit()`\n",
    "1. After the model has finished training, open a new  terminal, navigate to the directory that contains the newly created `logs` folder (or whatever we named it), and run the `tensorboard` command to throw up a local server running tensorboard (more on this later).  \n",
    "\n",
    "Now,We'll build a model that purposely underfits the data, and then examine it in Tensorboard.\n",
    "\n",
    "**_Challenge:_**   Build, compile, and fit a model with the following specifications:\n",
    "\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-10jt{font-weight:bold;font-size:32px;border-color:inherit;text-align:center;vertical-align:top}\n",
    ".tg .tg-tns0{font-weight:bold;font-size:32px;text-align:center;vertical-align:top}\n",
    ".tg .tg-we1k{font-size:32px;border-color:inherit;text-align:center;vertical-align:top}\n",
    ".tg .tg-j32n{font-size:32px;text-align:center;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-10jt\">Layer #</th>\n",
    "    <th class=\"tg-10jt\">Layer Type</th>\n",
    "    <th class=\"tg-10jt\"># of Neurons</th>\n",
    "    <th class=\"tg-tns0\">Activation Function</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-we1k\">Input</td>\n",
    "    <td class=\"tg-we1k\">N/A</td>\n",
    "    <td class=\"tg-we1k\">(784,)</td>\n",
    "    <td class=\"tg-j32n\">N/A</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-we1k\">Hidden Layer 1</td>\n",
    "    <td class=\"tg-we1k\">Dense</td>\n",
    "    <td class=\"tg-we1k\">5</td>\n",
    "    <td class=\"tg-j32n\">sigmoid</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-we1k\">Output</td>\n",
    "    <td class=\"tg-we1k\">Dense</td>\n",
    "    <td class=\"tg-we1k\">10</td>\n",
    "    <td class=\"tg-j32n\">Softmax</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This should take 3 lines of code.  Remember, you can declare the input layer when declaring the first hidden layer \n",
    "# by passing the 'input_shape=' parameter, with the parameter set to the number of neurons in your input layer.  \n",
    "underfit_model = Sequential()\n",
    "underfit_model.add(Dense(5, activation='sigmoid', input_shape=(784,)))\n",
    "underfit_model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll compile the model with the following parameters:\n",
    "\n",
    "`loss` = `'categorical_crossentropy'`  \n",
    "<br>\n",
    "`optimizer` = `'SGD'`  \n",
    "<br>\n",
    "`metrics` = `['accuracy']`\n",
    "\n",
    "**_Challenge:_** Call `model.compile()` and pass in the parameters above with their corresponding values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "underfit_model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll fit the model.  We're going to purposefully pass in parameters that will cause it further underfit (the shape of our neural network is too small, and will already cause it to underfit the data significantly).\n",
    "\n",
    "Before we call the model, we'll want to create a Tensorboard Callback to pass into the model.  The code to create a Tensorboard callback has already been provided for you in the cell.  For more information, see [Keras's documentation on Tensorboard Callbacks]()\n",
    "\n",
    "Parameters to pass into `underfit_model.fit()`:\n",
    "\n",
    "`X_train`\n",
    "\n",
    "`y_train`\n",
    "\n",
    "`batch_size` = `2048`\n",
    "\n",
    "`verbose` = `1`\n",
    "\n",
    "`epochs` = `2`\n",
    "\n",
    "`validation_data` = `(X_test, y_test)`\n",
    "\n",
    "`callbacks` = `[tb_callback]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 2.3172 - acc: 0.1550 - val_loss: 2.2829 - val_acc: 0.1921\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 2.2663 - acc: 0.2118 - val_loss: 2.2446 - val_acc: 0.2328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x127c20128>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_callback = TensorBoard(log_dir='./underfit_logs', histogram_freq=0, batch_size=512, write_graph=True, \n",
    "                          write_grads=False, write_images=False, embeddings_freq=0, \n",
    "                          embeddings_layer_names=None, embeddings_metadata=None)\n",
    "\n",
    "underfit_model.fit(X_train, y_train, batch_size=512, verbose=1, epochs=2, validation_data=(X_test, y_test), callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Examining Our Model in Tensorboard</h2></center>\n",
    "\n",
    "Go to your terminal, and navigate to the folder that contains this jupyter notebook.  Use `ls` to examine the contents of the folder--you'll see that the folder now contains a sub-directory called `underfit_logs`.  This is a folder that TensorFlow created for us to store the logs it writes during the training, so that we can visualize it in Tensorboard. If you don't have this folder, then Tensorboard won't work, because it won't have anything to visualize for us.  \n",
    "\n",
    "To run Tensorboard, type `tensorboard --logdir='underfit_logs'`.  \n",
    "\n",
    "You'll see a message saying `Starting Tensorboard at http://0.0.0.0:6006`--it may take a few seconds to Tensorboard to start up. Open a new tab in your web browser and navigate to that http address, and voila! You're in Tensorboard.  \n",
    "\n",
    "**_Challenge:_** Dig around in Tensorboard and see everything that it has to offer.  Take a look at a graphs for loss, accuracy, validation loss, and validation accuracy.  How can we use these graphs to check the fit of our model?\n",
    "\n",
    "<center><h3>Fixing Our Underfit Model</h3></center>\n",
    "\n",
    "Because our training accuracy and validation accuracy are absymally low, we can safely assume that our model has underfit the data.  Let's think critically about why this might be. Answer each of the following questions. \n",
    "\n",
    "**_1._**  Is our model too simple to capture the complexity in the dataset? How would changing the number of hidden layers in the model affect the model's ability to capture more complexity?  What about changing the number of neurons in 1 or more hidden layers?  What is the difference between adding a layer and adding more neurons to a layer?\n",
    "\n",
    "**_ANSWER:_**\n",
    "\n",
    "**_2._** The batch size is unreasonably high.  How would increasing or decreasing the batch size affect performance? How would it affect the runtime? Why do you think this is? (Bonus question--use Google to try and discover why we tend to use powers of 2 for batch size!)\n",
    "\n",
    "**_ANSWER:_**\n",
    "\n",
    "**_3._** The number of epochs is quite low. In terms of training, what does 1 full epoch mean? How does increasing the number of epochs afect performance?  How does it affect the runtime? Why?\n",
    "\n",
    "**_ANSWER:_**  \n",
    "\n",
    "**_4._** There are a few other hyperparameters we can tune.  For instance, we can use a different activation function, or a different optimizer.  Try out different activation functions/optimizers.  How does each affect performance? How about run time? Why do you think that is?\n",
    "\n",
    "**_ANSWER:_**\n",
    "\n",
    "<center><h3>Challenge: Tune the Underfit Model</h3></center>\n",
    "\n",
    "Test your answers and see if your intuition was correct! Play around the different hyperparameters discussed above, and see if you can achieve a better fit on this model.  \n",
    "\n",
    "**_IMPORTANT NOTE:_**  BE SCIENTIFIC IN YOUR APPROACH! Don't just change everything at once--then, you don't know what affect each change had.  Make **_one change at a time_**, and track the results of each change (think **_dependent_** and **_independent_** variables--its called data **_SCIENCE_** for a reason!)\n",
    "\n",
    "### Be sure to restart the kernel with every new iteration--otherwise, the changes will just piggyback off the weights of the old model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_48 (Dense)             (None, 784)               615440    \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 623,290\n",
      "Trainable params: 623,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Recreate the model from above, but with higher complexity.  Choose better hyperparameters to tune it!\n",
    "good_model = Sequential()\n",
    "good_model.add(Dense(784, activation='relu', input_shape=(784,)))\n",
    "good_model.add(Dense(10, activation='softmax'))\n",
    "good_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "good_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb_callback = TensorBoard(log_dir='./goodfit_logs', histogram_freq=0, batch_size=32, write_graph=True, \n",
    "                          write_grads=False, write_images=False, embeddings_freq=0, \n",
    "                          embeddings_layer_names=None, embeddings_metadata=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 44s 729us/step - loss: 0.1898 - acc: 0.9431 - val_loss: 0.1011 - val_acc: 0.9692\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 43s 713us/step - loss: 0.0755 - acc: 0.9770 - val_loss: 0.0747 - val_acc: 0.9764\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 47s 781us/step - loss: 0.0487 - acc: 0.9842 - val_loss: 0.0801 - val_acc: 0.9754\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 44s 739us/step - loss: 0.0348 - acc: 0.9882 - val_loss: 0.0845 - val_acc: 0.9745\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 43s 720us/step - loss: 0.0254 - acc: 0.9915 - val_loss: 0.0774 - val_acc: 0.9803\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 43s 711us/step - loss: 0.0221 - acc: 0.9928 - val_loss: 0.0691 - val_acc: 0.9810\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 44s 730us/step - loss: 0.0168 - acc: 0.9942 - val_loss: 0.0685 - val_acc: 0.9803\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 45s 754us/step - loss: 0.0139 - acc: 0.9952 - val_loss: 0.0934 - val_acc: 0.9774\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 43s 724us/step - loss: 0.0119 - acc: 0.9959 - val_loss: 0.0752 - val_acc: 0.9824\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 44s 730us/step - loss: 0.0116 - acc: 0.9962 - val_loss: 0.0843 - val_acc: 0.9809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x33803bbe0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_model.fit(X_train, y_train, batch_size=32, verbose=1, epochs=10, validation_data=(X_test, y_test), callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BenchMark\n",
    "1. - input 784 'sigmoid', output 10 'softmax'\n",
    "   - epochs 2, batch_size 512, optimizer SGD\n",
    "   - loss: 1.9837 - acc: 0.5941 - val_loss: 1.8759 - val_acc: 0.6782\n",
    "2. - input 784 'sigmoid', layer1 256 'sigmoid', output 10 'softmax'\n",
    "   - epochs 2, batch_size 512, optimizer SGD\n",
    "   - loss: 2.2651 - acc: 0.2058 - val_loss: 2.2534 - val_acc: 0.2298\n",
    "3. - input 784 'sigmoid', layer1 128 'sigmoid', output 10 'softmax'\n",
    "   - epochs 2, batch_size 512, optimizer SGD\n",
    "   - loss: 2.2662 - acc: 0.1948 - val_loss: 2.2551 - val_acc: 0.3374\n",
    "4. - input 784 'sigmoid', layer1 64 'sigmoid', output 10 'softmax'\n",
    "   - epochs 2, batch_size 512, optimizer SGD\n",
    "   - loss: 2.2701 - acc: 0.1963 - val_loss: 2.2580 - val_acc: 0.2520\n",
    "5. - input 784 'sigmoid', layer1 128 'sigmoid', layer1 64 'sigmoid', output 10 'softmax'\n",
    "   - epochs 2, batch_size 512, optimizer SGD\n",
    "   - loss: 2.3002 - acc: 0.1101 - val_loss: 2.2987 - val_acc: 0.1135\n",
    "6. - input 784 'sigmoid', output 10 'softmax'\n",
    "   - epochs 2, batch_size 256, optimizer SGD\n",
    "   - loss: 1.7241 - acc: 0.7164 - val_loss: 1.5515 - val_acc: 0.7904\n",
    "7. - input 784 'sigmoid', output 10 'softmax'\n",
    "   - epochs 2, batch_size 128, optimizer SGD\n",
    "   - loss: 1.3289 - acc: 0.7665 - val_loss: 1.1049 - val_acc: 0.8088\n",
    "8. - input 784 'sigmoid', output 10 'softmax'\n",
    "   - epochs 2, batch_size 64, optimizer SGD\n",
    "   - loss: 0.8995 - acc: 0.8229 - val_loss: 0.7147 - val_acc: 0.8507\n",
    "9. - input 784 'sigmoid', output 10 'softmax'\n",
    "   - epochs 2, batch_size 32, optimizer SGD\n",
    "   - loss: 0.6070 - acc: 0.8551 - val_loss: 0.4933 - val_acc: 0.8807\n",
    "10. - input 784 'sigmoid', output 10 'softmax'\n",
    "   - epochs 2, batch_size 16, optimizer SGD\n",
    "   - loss: 0.4493 - acc: 0.8791 - val_loss: 0.3806 - val_acc: 0.8936\n",
    "11. - input 784 'sigmoid', output 10 'softmax'\n",
    "   - epochs 2, batch_size 8, optimizer SGD\n",
    "   - loss: 0.3714 - acc: 0.8937 - val_loss: 0.3374 - val_acc: 0.9060\n",
    "12. - input 784 'sigmoid', output 10 'softmax'\n",
    "   - epochs 5, batch_size 16, optimizer SGD\n",
    "   - loss: 0.3336 - acc: 0.9043 - val_loss: 0.3217 - val_acc: 0.9056\n",
    "13. - input 784 'sigmoid', output 10 'softmax'\n",
    "   - epochs 10, batch_size 16, optimizer SGD\n",
    "   - 57s 953us/step loss: 0.2959 - acc: 0.9149 - val_loss: 0.2831 - val_acc: 0.9177\n",
    "14. - input 784 'sigmoid', output 10 'softmax'\n",
    "   - epochs 10, batch_size 32, optimizer SGD\n",
    "   - 35s 592us/step loss: 0.3271 - acc: 0.9063 - val_loss: 0.3095 - val_acc: 0.9112\n",
    "15. - input 784 'sigmoid', output 10 'softmax'\n",
    "   - epochs 30, batch_size 32, optimizer SGD\n",
    "   - 34s 564us/step - loss: 0.2751 - acc: 0.9215 - val_loss: 0.2702 - val_acc: 0.9229\n",
    "16. - input 784 'sigmoid', output 10 'softmax'\n",
    "   - epochs 50, batch_size 32, optimizer SGD\n",
    "   - 26s 434us/step - loss: 0.2430 - acc: 0.9320 - val_loss: 0.2432 - val_acc: 0.9324\n",
    "17. - input 784 'relu', output 10 'softmax'\n",
    "   - epochs 30, batch_size 32, optimizer SGD\n",
    "   - 23s 383us/step - loss: 0.0621 - acc: 0.9842 - val_loss: 0.0817 - val_acc: 0.9766\n",
    "18. - input 784 'relu', output 10 'softmax'\n",
    "   - epochs 30, batch_size 32, optimizer adam\n",
    "   - 47s 782us/step - loss: 0.0070 - acc: 0.9983 - val_loss: 0.1272 - val_acc: 0.9841\n",
    "### 19. - input 784 'relu', output 10 'softmax'\n",
    "###   - epochs 10, batch_size 32, optimizer adam\n",
    "###   - 44s 730us/step - loss: 0.0116 - acc: 0.9962 - val_loss: 0.0843 - val_acc: 0.9809"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Regularization with Dropout</h2></center>\n",
    "\n",
    "**_Dropout_** is a popular regularization technique in Deep Learning that helps us deal with overfitting.  The way dropout works is fairly intuitive, and implementing it in keras is dead simple.  \n",
    "\n",
    "Take the following network as an example:\n",
    "\n",
    "```python\n",
    "model2 = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "```\n",
    "We've added a **_Dropout Layer_** immediately after our first Dense layer. By passing in a value of `.5`, we've set the the dropout layer to give each neuron a 50% chance of returning 0 instead of the neurons actual output.  Essentially, every neuron has a 50% chance of \"dropping out\", hence the name.  This may sound like a bad idea at first, but it's actually pretty intuitive.  Consider the following example:\n",
    "\n",
    "## Dropout and Basketball\n",
    "\n",
    "You're a high school basketball coach, with a team consisting of average players.  After a few games, you notice that some players play really well when they're on the court together.  In practice as well as games, they spend all their time running plays together.  This works great with every teammate learning to play really with well a partner, co-adapting to each other's performance--that is, until certain players are sick or injured, and can't play.  Now, you lose games, because your players didn't get enough practice playing basketball on their own, or with other random partners.  They only play well with the players they co-adapted with.\n",
    "\n",
    "What do you do?\n",
    "\n",
    "Simple--you should just randomly bench certain players during practice, forcing each player to learn and adapt to the play well with anyone else on the court. \n",
    "\n",
    "This is exactly the type of situation that dropout prevents in a neural network.  It's actually a bit more complex, because with Neural Networks, we don't actually know which neurons will or won't be the \"star players\".  Instead, we give them all an equal chance of dropping out, forcing the remaining neurons in the network to \"learn\" the correct weights with the teammates they have. \n",
    "\n",
    "Importantly, dropout layers are only turned on during the training phase. During testing, all neurons are present, and none will drop out.  This makes sense, too--it's one thing to bench players during practice, but we need everyone ready to contribute during the game.  \n",
    "\n",
    "This is a somewhat simplified explanation of dropout--to learn more about why it works, try starting with [this wonderful medium article on the topic](https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5).\n",
    "\n",
    "<center><img src='img/dropout.png' height=60% width=60%></center>\n",
    "\n",
    "**_Challenge:_** Modify the network you wrote above to include dropout layers between every dense layer.  How does this affect your training accuracy? How does this affect your test accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_54 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "# Write a network that includes dropout layers below!\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(128, activation='relu', input_shape=(784,)))\n",
    "model2.add(Dropout(.5))\n",
    "model2.add(Dense(10, activation='softmax'))\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb_callback = TensorBoard(log_dir='./dropoutfit_logs', histogram_freq=0, batch_size=32, write_graph=True, \n",
    "                          write_grads=False, write_images=False, embeddings_freq=0, \n",
    "                          embeddings_layer_names=None, embeddings_metadata=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 12s 201us/step - loss: 0.3980 - acc: 0.8835 - val_loss: 0.1787 - val_acc: 0.9480\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 12s 195us/step - loss: 0.2223 - acc: 0.9347 - val_loss: 0.1238 - val_acc: 0.9620\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 12s 208us/step - loss: 0.1862 - acc: 0.9447 - val_loss: 0.1052 - val_acc: 0.9681\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 13s 222us/step - loss: 0.1617 - acc: 0.9514 - val_loss: 0.0970 - val_acc: 0.9706\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 12s 203us/step - loss: 0.1489 - acc: 0.9553 - val_loss: 0.0919 - val_acc: 0.9715\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.1377 - acc: 0.9575 - val_loss: 0.0917 - val_acc: 0.9735\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 13s 212us/step - loss: 0.1320 - acc: 0.9596 - val_loss: 0.0825 - val_acc: 0.9761\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 12s 204us/step - loss: 0.1216 - acc: 0.9624 - val_loss: 0.0852 - val_acc: 0.9761\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 14s 233us/step - loss: 0.1181 - acc: 0.9637 - val_loss: 0.0857 - val_acc: 0.9758\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 13s 219us/step - loss: 0.1167 - acc: 0.9642 - val_loss: 0.0842 - val_acc: 0.9768\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 12s 202us/step - loss: 0.1084 - acc: 0.9660 - val_loss: 0.0798 - val_acc: 0.9773\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 13s 212us/step - loss: 0.1061 - acc: 0.9658 - val_loss: 0.0817 - val_acc: 0.9779\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 12s 204us/step - loss: 0.1022 - acc: 0.9678 - val_loss: 0.0812 - val_acc: 0.9768\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 12s 204us/step - loss: 0.0993 - acc: 0.9682 - val_loss: 0.0815 - val_acc: 0.9771\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 12s 205us/step - loss: 0.0971 - acc: 0.9683 - val_loss: 0.0786 - val_acc: 0.9776\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 12s 203us/step - loss: 0.0967 - acc: 0.9695 - val_loss: 0.0819 - val_acc: 0.9783\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 12s 202us/step - loss: 0.0955 - acc: 0.9680 - val_loss: 0.0805 - val_acc: 0.9795\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 12s 203us/step - loss: 0.0931 - acc: 0.9693 - val_loss: 0.0814 - val_acc: 0.9781\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 12s 201us/step - loss: 0.0907 - acc: 0.9706 - val_loss: 0.0820 - val_acc: 0.9788\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 12s 202us/step - loss: 0.0889 - acc: 0.9716 - val_loss: 0.0815 - val_acc: 0.9775\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 12s 203us/step - loss: 0.0846 - acc: 0.9724 - val_loss: 0.0830 - val_acc: 0.9791\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 12s 202us/step - loss: 0.0847 - acc: 0.9727 - val_loss: 0.0848 - val_acc: 0.9766\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 12s 201us/step - loss: 0.0816 - acc: 0.9736 - val_loss: 0.0822 - val_acc: 0.9785\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 13s 217us/step - loss: 0.0846 - acc: 0.9725 - val_loss: 0.0868 - val_acc: 0.9778\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 12s 206us/step - loss: 0.0797 - acc: 0.9735 - val_loss: 0.0839 - val_acc: 0.9778\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 12s 204us/step - loss: 0.0795 - acc: 0.9735 - val_loss: 0.0852 - val_acc: 0.9794\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 13s 213us/step - loss: 0.0805 - acc: 0.9736 - val_loss: 0.0913 - val_acc: 0.9784\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 13s 212us/step - loss: 0.0795 - acc: 0.9733 - val_loss: 0.0844 - val_acc: 0.9779\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 13s 214us/step - loss: 0.0812 - acc: 0.9731 - val_loss: 0.0899 - val_acc: 0.9778\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 12s 205us/step - loss: 0.0729 - acc: 0.9755 - val_loss: 0.0863 - val_acc: 0.9790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x33e188588>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, y_train, batch_size=32, verbose=1, epochs=30, validation_data=(X_test, y_test), callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BenchMark\n",
    "1. - input 128 'sigmoid', output 10 'softmax'\n",
    "   - epochs 10, batch_size 32, optimizer adam\n",
    "   - 12s 197us/step - loss: 0.1151 - acc: 0.9636 - val_loss: 0.0839 - val_acc: 0.9764\n",
    "2. - input 128 'sigmoid', output 10 'softmax'\n",
    "   - epochs 30, batch_size 32, optimizer adam\n",
    "   - 12s 205us/step - loss: 0.0729 - acc: 0.9755 - val_loss: 0.0863 - val_acc: 0.9790"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
